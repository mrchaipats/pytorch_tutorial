{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef5a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccba837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# num_classes = 2\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cfdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone.out_channels = 1280\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "\n",
    "# model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator,\n",
    "#                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47102b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4337a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "git clone https://github.com/pytorch/vision.git\n",
    "cd vision\n",
    "git checkout v0.3.0\n",
    "\n",
    "cp references/detection/utils.py ../\n",
    "cp references/detection/transforms.py ../\n",
    "cp references/detection/coco_eval.py ../\n",
    "cp references/detection/engine.py ../\n",
    "cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e0dd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb24e1ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = PennFudanDataset(\n",
    "    'PennFudanPed',\n",
    "    get_transform(train=True)\n",
    ")\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)\n",
    "\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099aab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    num_classes = 2\n",
    "    dataset = PennFudanDataset(\n",
    "        'PennFudanPed',\n",
    "        get_transform(train=True)\n",
    "    )\n",
    "    dataset_test = PennFudanDataset(\n",
    "        'PennFudanPed',\n",
    "        get_transform(train=False)\n",
    "    )\n",
    "    \n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=utils.collate_fn\n",
    "    )\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=utils.collate_fn\n",
    "    )\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, \n",
    "        lr=0.005, \n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=3,\n",
    "        gamma=0.1\n",
    "    )\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63532332",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/60]  eta: 0:07:11  lr: 0.000090  loss: 3.3793 (3.3793)  loss_classifier: 0.5394 (0.5394)  loss_box_reg: 0.5126 (0.5126)  loss_mask: 2.3122 (2.3122)  loss_objectness: 0.0110 (0.0110)  loss_rpn_box_reg: 0.0040 (0.0040)  time: 7.1841  data: 0.0319  max mem: 0\n",
      "Epoch: [0]  [10/60]  eta: 0:04:05  lr: 0.000936  loss: 1.2328 (1.7751)  loss_classifier: 0.3406 (0.3567)  loss_box_reg: 0.1953 (0.2654)  loss_mask: 0.7178 (1.1272)  loss_objectness: 0.0204 (0.0223)  loss_rpn_box_reg: 0.0021 (0.0036)  time: 4.9186  data: 0.0238  max mem: 0\n",
      "Epoch: [0]  [20/60]  eta: 0:03:12  lr: 0.001783  loss: 0.8333 (1.2288)  loss_classifier: 0.1682 (0.2527)  loss_box_reg: 0.1900 (0.2467)  loss_mask: 0.2994 (0.7091)  loss_objectness: 0.0131 (0.0154)  loss_rpn_box_reg: 0.0021 (0.0048)  time: 4.6906  data: 0.0241  max mem: 0\n",
      "Epoch: [0]  [30/60]  eta: 0:02:26  lr: 0.002629  loss: 0.5062 (0.9982)  loss_classifier: 0.0923 (0.1967)  loss_box_reg: 0.1988 (0.2365)  loss_mask: 0.2076 (0.5471)  loss_objectness: 0.0046 (0.0127)  loss_rpn_box_reg: 0.0041 (0.0052)  time: 4.8447  data: 0.0235  max mem: 0\n",
      "Epoch: [0]  [40/60]  eta: 0:01:38  lr: 0.003476  loss: 0.4757 (0.8796)  loss_classifier: 0.0679 (0.1659)  loss_box_reg: 0.2321 (0.2413)  loss_mask: 0.1656 (0.4563)  loss_objectness: 0.0030 (0.0105)  loss_rpn_box_reg: 0.0056 (0.0056)  time: 5.0579  data: 0.0231  max mem: 0\n",
      "Epoch: [0]  [50/60]  eta: 0:00:49  lr: 0.004323  loss: 0.4489 (0.7869)  loss_classifier: 0.0579 (0.1430)  loss_box_reg: 0.2166 (0.2277)  loss_mask: 0.1806 (0.4016)  loss_objectness: 0.0017 (0.0087)  loss_rpn_box_reg: 0.0055 (0.0059)  time: 5.1596  data: 0.0252  max mem: 0\n",
      "Epoch: [0]  [59/60]  eta: 0:00:05  lr: 0.005000  loss: 0.3983 (0.7296)  loss_classifier: 0.0518 (0.1306)  loss_box_reg: 0.1556 (0.2167)  loss_mask: 0.1789 (0.3683)  loss_objectness: 0.0014 (0.0078)  loss_rpn_box_reg: 0.0055 (0.0061)  time: 5.2369  data: 0.0256  max mem: 0\n",
      "Epoch: [0] Total time: 0:05:01 (5.0287 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     56\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms it!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_python3_8/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep_learning/learning_pytorch/notebooks/image_and_video/engine.py:86\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m image)\n\u001b[1;32m     84\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 86\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m model_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     88\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_python3_8/lib/python3.8/site-packages/torch/cuda/__init__.py:493\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Waits for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_python3_8/lib/python3.8/site-packages/torch/cuda/__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0050f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
