{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473583a8",
   "metadata": {},
   "source": [
    "Data-efficient Image Transformers (DeiT) is a Vision Transformer model trained on ImageNet for image classification.\n",
    "\n",
    "CNNs requires millions of images for training to achieve the SOTA results.\n",
    "\n",
    "DeiT is a vision transformer model that requires a lot less data and computing resources for training to compete with the leading CNNs in performing image classification, which is made possible by two key components of DeiT:\n",
    "\n",
    "    - Data augmentation that simulates training on a much larger dataset;\n",
    "    \n",
    "    - Native distillation that allows the transformer network to learn from a CNN's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620820b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bcp/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "model = torch.hub.load(\n",
    "    'facebookresearch/deit:main', \n",
    "    'deit_base_patch16_224', \n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        IMAGENET_DEFAULT_MEAN,\n",
    "        IMAGENET_DEFAULT_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "img = Image.open(\n",
    "    requests.get(\n",
    "        \"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\",\n",
    "        stream=True).raw\n",
    ")\n",
    "\n",
    "img = transform(img)[None,] # [None,] add dimension at the beginning\n",
    "out = model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4644b",
   "metadata": {},
   "source": [
    "### Scripting DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8801eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bcp/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    'facebookresearch/deit:main', \n",
    "    'deit_base_patch16_224', \n",
    "    pretrained=True\n",
    ")\n",
    "# 346MB\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save('fbdeit_scripted.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9bcaab",
   "metadata": {},
   "source": [
    "### Quantizing DeiT\n",
    "\n",
    "apply dynamic-quantization to reduce the trained model size while keeping the inference accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625d6e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W TensorImpl.h:1463] Warning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Use 'fbgemm' for server inference\n",
    "# and 'qnnpack' for mobile inference\n",
    "backend = 'qnnpack'\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    qconfig_spec={torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 89MB\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "scripted_quantized_model.save('fbdeit_scripted_quantized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf12bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16ebc4",
   "metadata": {},
   "source": [
    "### Optimizing DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7937dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "# 89MB\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
    "optimized_scripted_quantized_model.save('fbdeit_optimized_scripted_quantized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "150cc125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = optimized_scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab565b",
   "metadata": {},
   "source": [
    "### Using Lite Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3c1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89MB\n",
    "optimized_scripted_quantized_model._save_for_lite_interpreter(\n",
    "    'fbdeit_optimized_scripted_quantized_lite.ptl'\n",
    ")\n",
    "ptl = torch.jit.load('fbdeit_optimized_scripted_quantized_lite.ptl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f02a3",
   "metadata": {},
   "source": [
    "### Comparing Inference Speed\n",
    "\n",
    "'fbgemm' should be use for comparing inference on Notebook, however M1 is not supporting 'fbgemm' quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16810f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model: 123.96ms\n",
      "scripted model: 253.00ms\n",
      "scripted & quantized model: 197.53ms\n",
      "scripted & quantized & optimized model: 175.56ms\n",
      "lite model: 161.18ms\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
    "    out = model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
    "    out = scripted_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
    "    out = scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
    "    out = optimized_scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
    "    out = ptl(img)\n",
    "\n",
    "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
    "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
    "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "379ede85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Model Inference Time Reduction\n",
      "0                          original model       123.96ms        0%\n",
      "1                          scripted model       253.00ms  -104.10%\n",
      "2              scripted & quantized model       197.53ms   -59.35%\n",
      "3  scripted & quantized & optimized model       175.56ms   -41.63%\n",
      "4                              lite model       161.18ms   -30.03%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'original model', \n",
    "        'scripted model', \n",
    "        'scripted & quantized model',\n",
    "        'scripted & quantized & optimized model', \n",
    "        'lite model'\n",
    "    ]\n",
    "})\n",
    "\n",
    "df = pd.concat([\n",
    "    df,\n",
    "    pd.DataFrame([\n",
    "            [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
    "            [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
    "             \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "            [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
    "             \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "            [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
    "             \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "            [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
    "             \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]\n",
    "        ],\n",
    "        columns=['Inference Time', 'Reduction']\n",
    "    )\n",
    "], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdcc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
